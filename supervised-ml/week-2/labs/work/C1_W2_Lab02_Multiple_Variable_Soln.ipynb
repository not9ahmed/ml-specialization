{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Lab: Multiple Variable Linear Regression\n",
    "\n",
    "In this lab, you will extend the data structures and previously developed routines to support multiple features. Several routines are updated making the lab appear lengthy, but it makes minor adjustments to previous routines making it quick to review.\n",
    "# Outline\n",
    "- [&nbsp;&nbsp;1.1 Goals](#toc_15456_1.1)\n",
    "- [&nbsp;&nbsp;1.2 Tools](#toc_15456_1.2)\n",
    "- [&nbsp;&nbsp;1.3 Notation](#toc_15456_1.3)\n",
    "- [2 Problem Statement](#toc_15456_2)\n",
    "- [&nbsp;&nbsp;2.1 Matrix X containing our examples](#toc_15456_2.1)\n",
    "- [&nbsp;&nbsp;2.2 Parameter vector w, b](#toc_15456_2.2)\n",
    "- [3 Model Prediction With Multiple Variables](#toc_15456_3)\n",
    "- [&nbsp;&nbsp;3.1 Single Prediction element by element](#toc_15456_3.1)\n",
    "- [&nbsp;&nbsp;3.2 Single Prediction, vector](#toc_15456_3.2)\n",
    "- [4 Compute Cost With Multiple Variables](#toc_15456_4)\n",
    "- [5 Gradient Descent With Multiple Variables](#toc_15456_5)\n",
    "- [&nbsp;&nbsp;5.1 Compute Gradient with Multiple Variables](#toc_15456_5.1)\n",
    "- [&nbsp;&nbsp;5.2 Gradient Descent With Multiple Variables](#toc_15456_5.2)\n",
    "- [6 Congratulations](#toc_15456_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.1\"></a>\n",
    "## 1.1 Goals\n",
    "- Extend our regression model  routines to support multiple features\n",
    "    - Extend data structures to support multiple features\n",
    "    - Rewrite prediction, cost and gradient routines to support multiple features\n",
    "    - Utilize NumPy `np.dot` to vectorize their implementations for speed and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.2\"></a>\n",
    "## 1.2 Tools\n",
    "In this lab, we will make use of: \n",
    "- NumPy, a popular library for scientific computing\n",
    "- Matplotlib, a popular library for plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing copy module which create shallow and deep copy of objects, and math library for math operations\n",
    "import copy, math\n",
    "\n",
    "# importing numpy for vectroized implmentations\n",
    "import numpy as np\n",
    "\n",
    "# importing matplot to plot model, gradient descent etc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use custom style sheet\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_1.3\"></a>\n",
    "## 1.3 Notation\n",
    "Here is a summary of some of the notation you will encounter, updated for multiple features.  \n",
    "\n",
    "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example matrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2\"></a>\n",
    "# 2 Problem Statement\n",
    "\n",
    "You will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below.  Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n",
    "You will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.  \n",
    "\n",
    "Please run the following code cell to create your `X_train` and `y_train` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will create a matrix X with dimension  m X n ==> m rows X n columns ==> example X features\n",
    "#      |    $x_(0)    |   x_(1)   |   x_(2)    |   x_(3) |\n",
    "# x^(0)|    \n",
    "# x^(1)|\n",
    "# superscript ==> ^(i) ==> example number\n",
    "# subscript ==> _(j) ==> element\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "\n",
    "\n",
    "# will create the column of features y ==> vector\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2.1\"></a>\n",
    "## 2.1 Matrix X containing our examples\n",
    "Similar to the table above, examples are stored in a NumPy matrix `X_train`. Each row of the matrix represents one example. When you have $m$ training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\\mathbf{X}$ is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
    "\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "notation:\n",
    "- $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
    "- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.  \n",
    "\n",
    "Display the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (3, 4), X Type:<class 'numpy.ndarray'>)\n",
      "[[2104    5    1   45]\n",
      " [1416    3    2   40]\n",
      " [ 852    2    1   35]]\n",
      "y Shape: (3,), y Type:<class 'numpy.ndarray'>)\n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "# data is stored in numpy array/matrix\n",
    "\n",
    "# X_train shape will be (3, 4) ==> (3 rows, 4 columns) ==> (3 examples, 4 features)\n",
    "# type wil be ndarray\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(X_train)\n",
    "\n",
    "\n",
    "# Y_train shape will be (3, ) ==> (3 rows, 1 column) ==> (3 examples targets, )\n",
    "# type wil be ndarray\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_2.2\"></a>\n",
    "## 2.2 Parameter vector w, b\n",
    "\n",
    "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "  - in our dataset, n is 4.\n",
    "  - notionally, we draw this as a column vector\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $b$ is a scalar parameter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, $\\mathbf{w}$ and $b$ will be loaded with some initial selected values that are near the optimal. $\\mathbf{w}$ is a 1-D NumPy vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (4,), b_init type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "\n",
    "# w shape will be (4,) (4 rows, 1 column)\n",
    "# b type will scalar/ number\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3\"></a>\n",
    "# 3 Model Prediction With Multiple Variables\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n",
    "To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3.1\"></a>\n",
    "## 3.1 Single Prediction element by element\n",
    "Our previous prediction multiplied one feature value by one parameter and added a bias parameter. A direct extension of our previous implementation of prediction to multiple features would be to implement (1) above using loop over each element, performing the multiply with its parameter and then adding the bias parameter at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters    \n",
    "      b (scalar):  model parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # can also use (length(x))\n",
    "    n = x.shape[0]\n",
    "\n",
    "    # p is for storing final prediction\n",
    "    p = 0\n",
    "    for i in range(n):\n",
    "        \n",
    "        # doing x_i * w_i\n",
    "        p_i = x[i] * w[i]  \n",
    "\n",
    "        # storing to final p\n",
    "        p = p + p_i\n",
    "\n",
    "    # returning the final/total prediction from all W * X vectors\n",
    "    p = p + b                \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "f_wb shape (), prediction: 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "# will get row and all the columns \":\" this what it means\n",
    "# x^(0) = [2104    5    1   45]\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "# w_init, b_init are from the previous cells\n",
    "# prediction: 459.9999976194083 which close to the first example!\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the shape of `x_vec`. It is a 1-D NumPy vector with 4 elements, (4,). The result, `f_wb` is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3.2\"></a>\n",
    "## 3.2 Single Prediction, vector\n",
    "\n",
    "Noting that equation (1) above can be implemented using the dot product as in (2) above. We can make use of vector operations to speed up predictions.\n",
    "\n",
    "Recall from the Python/Numpy lab that NumPy `np.dot()`[[link](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)] can be used to perform a vector dot product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # vectorized version using numpy\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "f_wb shape (), prediction: 459.99999761940825\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data which is the first example\n",
    "# x^(0) = [2104    5    1   45]\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "# prediction: 459.99999761940825 which is close to the fist example\n",
    "f_wb = predict(x_vec,w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results and shapes are the same as the previous version which used looping. Going forward, `np.dot` will be used for these operations. The prediction is now a single statement. Most routines will implement it directly rather than calling a separate predict routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_4\"></a>\n",
    "# 4 Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of equations (3) and (4). Note that this uses a *standard pattern for this course* where a for loop over all `m` examples is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    # m is the number of rows ==> examples\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # J(w,b) which will be used to store the final/total cost\n",
    "    cost = 0.0\n",
    "\n",
    "    # looping over the examples from 0 till m-1\n",
    "    for i in range(m):\n",
    "        \n",
    "        # this will compute prediction for single row X[0] ==> ([2104  5 1 45] * W[0] weights) + b\n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "\n",
    "        # will take ^2 then add up to final cost\n",
    "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
    "    \n",
    "    # computes the outer sigma statement 1/2m\n",
    "    cost = cost / (2 * m)                      #scalar    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 1.5578904880036537e-12\n"
     ]
    }
   ],
   "source": [
    "# Compute and display cost using our pre-chosen optimal parameters.\n",
    "\n",
    "# will take X matrix that represents the examples and features\n",
    "# y vector for the target values\n",
    "# w_init vector, b_init scalar are declared in before cells\n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**: Cost at optimal w : 1.5578904045996674e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "# 5 Gradient Descent With Multiple Variables\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5.1\"></a>\n",
    "## 5.1 Compute Gradient with Multiple Variables\n",
    "An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an\n",
    "- outer loop over all m examples. \n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
    "    - in a second loop over all n features:\n",
    "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "\n",
    "    # create np array of zeros with shape (n,0) ==> 1D [0 1 .. n-1], vector\n",
    "    dj_dw = np.zeros((n,))\n",
    "\n",
    "    # scalar for bias\n",
    "    dj_db = 0.\n",
    "\n",
    "    # loop from 0... n-1\n",
    "    for i in range(m):\n",
    "        \n",
    "\n",
    "        # used here to avoid repeating the statement\n",
    "        # compute the error by the expression inside sigma ==> f(x) - y ==> prediction - actual ==> f_wb(x_i) - y_i\n",
    "        # f_wb(x_i) ==> W^(i) * X^(i) + b\n",
    "        # f_wb(x_i) = (np.dot(X[i], w) + b) \n",
    "        err = (np.dot(X[i], w) + b) - y[i]\n",
    "\n",
    "        # for 0 till n-1\n",
    "        # computes the total d/dw J term which is the gradient descent\n",
    "        for j in range(n):                         \n",
    "            \n",
    "            # will take one w gradient derivative (scalar) + one b gradient derivative (scalar) * x_j^(i) which is (scalar)\n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
    "\n",
    "        \n",
    "\n",
    "        # computes the total cost for the bias\n",
    "        dj_db = dj_db + err\n",
    "        \n",
    "    # computes the final gradient for w and b then * 1/m which is outside the sigma expression\n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "    \n",
    "    # returns a tupple of bias gradient and weight gradient\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: -1.673925169143331e-06\n",
      "dj_dw at initial w,b: \n",
      " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]\n"
     ]
    }
   ],
   "source": [
    "# Compute and display gradient\n",
    "# X_train is matrix for examples and features\n",
    "# y_train is the vector for target values\n",
    "# w_init vector, b_init scalar are declared in before cells\n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**:   \n",
    "dj_db at initial w,b: -1.6739251122999121e-06  \n",
    "dj_dw at initial w,b:   \n",
    " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5.2\"></a>\n",
    "## 5.2 Gradient Descent With Multiple Variables\n",
    "The routine below implements equation (5) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "\n",
    "    # creates a deep copy of the original w_in\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "\n",
    "    # to store init b\n",
    "    b = b_in\n",
    "    \n",
    "\n",
    "    # will start from 0 till num_iters - 1\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        # will call the function compute_gradient which return gradient for w and b\n",
    "        dj_db, dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        # this will update the parameters w, b\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "      \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        # compute_cost call to compute the cost function by calling \n",
    "        # then add it to to J_history array for plotting\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        # num_iters = 1000\n",
    "        # (1000/10)= 100\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell you will test the implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2529.46   \n",
      "Iteration  100: Cost   695.99   \n",
      "Iteration  200: Cost   694.92   \n",
      "Iteration  300: Cost   693.86   \n",
      "Iteration  400: Cost   692.81   \n",
      "Iteration  500: Cost   691.77   \n",
      "Iteration  600: Cost   690.73   \n",
      "Iteration  700: Cost   689.71   \n",
      "Iteration  800: Cost   688.70   \n",
      "Iteration  900: Cost   687.69   \n",
      "\n",
      "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07] \n",
      "\n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "\n",
    "# wll create np 0 array with same shape as w_init\n",
    "# [0. 0. 0. 0.]\n",
    "initial_w = np.zeros_like(w_init)\n",
    "\n",
    "# self explained\n",
    "initial_b = 0.\n",
    "\n",
    "# some gradient descent settings\n",
    "# will run the gradient this many times\n",
    "iterations = 1000\n",
    "\n",
    "# 0.0000005\n",
    "alpha = 5.0e-7\n",
    "\n",
    "\n",
    "# run gradient descent\n",
    "# returns the optimal w and b\n",
    "# also J_hist which is list of cost\n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"\\nb,w found by gradient descent: {b_final:0.2f},{w_final} \\n\")\n",
    "\n",
    "# X_train.shape ==> (3, ) ==> (3 rows, 1 column) ==> 3 examples\n",
    "# _ means ignore the second value\n",
    "m,_ = X_train.shape\n",
    "\n",
    "# will compute the prediction and the actual value by taking the X_train dataset\n",
    "# from the printing it can be seen that it was still not accurate \n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**:    \n",
    "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07]   \n",
    "prediction: 426.19, target value: 460  \n",
    "prediction: 286.17, target value: 232  \n",
    "prediction: 171.47, target value: 178  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[695.990315835203, 695.979573101264, 695.9688313105621, 695.9580904630129, 695.9473505585346, 695.9366115970407, 695.9258735784491, 695.9151365026752, 695.9044003696362, 695.8936651792473, 695.8829309314256, 695.872197626086, 695.8614652631454, 695.8507338425202, 695.8400033641266, 695.8292738278801, 695.8185452336974, 695.8078175814954, 695.7970908711899, 695.7863651026965, 695.7756402759309, 695.76491639081, 695.754193447253, 695.7434714451714, 695.7327503844836, 695.7220302651052, 695.711311086955, 695.700592849947, 695.6898755539966, 695.6791591990242, 695.6684437849409, 695.657729311666, 695.6470157791151, 695.6363031872074, 695.6255915358547, 695.6148808249744, 695.6041710544852, 695.5934622243018, 695.5827543343413, 695.5720473845181, 695.5613413747514, 695.5506363049539, 695.5399321750466, 695.529228984944, 695.5185267345627, 695.507825423818, 695.4971250526263, 695.4864256209065, 695.4757271285713, 695.4650295755395, 695.4543329617287, 695.4436372870545, 695.4329425514314, 695.4222487547762, 695.4115558970096, 695.4008639780435, 695.390172997797, 695.3794829561847, 695.3687938531222, 695.3581056885309, 695.3474184623229, 695.3367321744148, 695.3260468247273, 695.3153624131702, 695.304678939669, 695.2939964041321, 695.2833148064806, 695.272634146629, 695.2619544244945, 695.251275639995, 695.2405977930457, 695.2299208835634, 695.2192449114644, 695.2085698766673, 695.1978957790865, 695.1872226186423, 695.1765503952438, 695.1658791088134, 695.1552087592685, 695.1445393465234, 695.1338708704944, 695.1232033311012, 695.1125367282548, 695.1018710618796, 695.0912063318877, 695.0805425381963, 695.0698796807218, 695.0592177593826, 695.0485567740919, 695.037896724772, 695.0272376113352, 695.0165794336989, 695.0059221917808, 694.9952658854977, 694.9846105147659, 694.9739560795055, 694.9633025796273, 694.9526500150527, 694.9419983856951, 694.9313476914762, 694.9206979323061, 694.9100491081077, 694.8994012187962, 694.8887542642861, 694.8781082444954, 694.8674631593436, 694.8568190087449, 694.846175792617, 694.8355335108778, 694.8248921634419, 694.8142517502251, 694.8036122711505, 694.7929737261288, 694.7823361150794, 694.7716994379189, 694.7610636945674, 694.7504288849356, 694.7397950089447, 694.7291620665114, 694.7185300575501, 694.707898981981, 694.6972688397204, 694.6866396306833, 694.6760113547898, 694.6653840119551, 694.6547576020959, 694.6441321251281, 694.6335075809722, 694.6228839695444, 694.6122612907579, 694.6016395445366, 694.5910187307896, 694.5803988494395, 694.5697799004016, 694.5591618835941, 694.5485447989321, 694.5379286463354, 694.527313425718, 694.5166991369975, 694.5060857800959, 694.4954733549217, 694.484861861402, 694.4742512994454, 694.4636416689738, 694.453032969904, 694.442425202149, 694.4318183656311, 694.4212124602672, 694.4106074859723, 694.400003442663, 694.3894003302571, 694.3787981486743, 694.3681968978293, 694.3575965776389, 694.3469971880214, 694.3363987288964, 694.3258012001766, 694.3152046017825, 694.3046089336294, 694.2940141956369, 694.283420387721, 694.2728275097988, 694.2622355617855, 694.2516445436023, 694.2410544551648, 694.2304652963913, 694.2198770671961, 694.209289767499, 694.1987033972183, 694.1881179562691, 694.1775334445716, 694.1669498620381, 694.1563672085904, 694.1457854841437, 694.1352046886182, 694.1246248219281, 694.1140458839931, 694.1034678747292, 694.0928907940528, 694.0823146418842, 694.0717394181378, 694.0611651227345, 694.0505917555871, 694.0400193166196, 694.0294478057409, 694.0188772228771, 694.0083075679412, 693.9977388408514, 693.9871710415223, 693.9766041698766, 693.9660382258284, 693.9554732092975, 693.9449091201965, 693.9343459584501, 693.9237837239717, 693.9132224166774, 693.9026620364898, 693.8921025833203, 693.8815440570917, 693.8709864577195, 693.8604297851192, 693.849874039211, 693.8393192199119, 693.8287653271401, 693.8182123608117, 693.8076603208468, 693.7971092071595, 693.7865590196706, 693.7760097582981, 693.7654614229535, 693.7549140135617, 693.7443675300368, 693.7338219722978, 693.7232773402616, 693.7127336338476, 693.7021908529713, 693.6916489975492, 693.6811080675029, 693.6705680627487, 693.6600289832008, 693.6494908287809, 693.6389535994064, 693.6284172949946, 693.6178819154626, 693.6073474607274, 693.5968139307097, 693.5862813253265, 693.575749644492, 693.5652188881276, 693.5546890561482, 693.5441601484757, 693.5336321650246, 693.5231051057123, 693.5125789704616, 693.5020537591859, 693.4915294718011, 693.4810061082288, 693.4704836683887, 693.459962152192, 693.4494415595605, 693.4389218904151, 693.4284031446691, 693.4178853222414, 693.4073684230497, 693.3968524470138, 693.386337394049, 693.375823264077, 693.3653100570104, 693.3547977727727, 693.3442864112784, 693.3337759724437, 693.3232664561916, 693.3127578624357, 693.3022501910963, 693.2917434420903, 693.2812376153378, 693.270732710755, 693.2602287282592, 693.249725667769, 693.2392235292045, 693.2287223124803, 693.218222017516, 693.2077226442292, 693.1972241925397, 693.1867266623625, 693.1762300536199, 693.165734366227, 693.1552396001007, 693.1447457551623, 693.1342528313266, 693.1237608285122, 693.1132697466406, 693.1027795856274, 693.0922903453911, 693.0818020258492, 693.0713146269203, 693.0608281485228, 693.0503425905744, 693.0398579529933, 693.0293742356984, 693.0188914386063, 693.008409561638, 692.9979286047069, 692.987448567735, 692.97696945064, 692.9664912533408, 692.9560139757522, 692.9455376177975, 692.9350621793895, 692.9245876604514, 692.9141140608964, 692.903641380646, 692.8931696196195, 692.8826987777305, 692.8722288549025, 692.8617598510515, 692.8512917660968, 692.8408245999527, 692.8303583525416, 692.8198930237817, 692.8094286135905, 692.7989651218858, 692.7885025485858, 692.7780408936093, 692.7675801568735, 692.7571203382981, 692.7466614378035, 692.7362034553029, 692.7257463907182, 692.7152902439693, 692.7048350149711, 692.6943807036436, 692.683927309904, 692.6734748336711, 692.6630232748639, 692.6525726334006, 692.6421229092001, 692.6316741021815, 692.6212262122591, 692.6107792393578, 692.6003331833917, 692.5898880442792, 692.5794438219397, 692.5690005162918, 692.5585581272557, 692.5481166547474, 692.5376760986857, 692.5272364589897, 692.5167977355758, 692.5063599283673, 692.4959230372779, 692.4854870622286, 692.4750520031386, 692.4646178599249, 692.4541846325055, 692.4437523207989, 692.433320924728, 692.4228904442061, 692.4124608791539, 692.4020322294887, 692.391604495131, 692.381177676, 692.3707517720119, 692.3603267830858, 692.349902709142, 692.3394795500958, 692.3290573058708, 692.3186359763799, 692.308215561547, 692.2977960612889, 692.2873774755209, 692.2769598041688, 692.2665430471429, 692.2561272043699, 692.2457122757633, 692.2352982612434, 692.2248851607279, 692.2144729741362, 692.2040617013877, 692.1936513424012, 692.1832418970938, 692.1728333653855, 692.1624257471973, 692.1520190424426, 692.1416132510459, 692.1312083729191, 692.1208044079873, 692.1104013561672, 692.0999992173769, 692.0895979915376, 692.0791976785636, 692.0687982783787, 692.058399790898, 692.0480022160423, 692.0376055537303, 692.0272098038778, 692.0168149664091, 692.0064210412406, 691.9960280282895, 691.9856359274768, 691.9752447387201, 691.9648544619403, 691.954465097052, 691.9440766439803, 691.9336891026369, 691.9233024729471, 691.9129167548252, 691.9025319481948, 691.8921480529701, 691.8817650690744, 691.871382996423, 691.8610018349365, 691.8506215845335, 691.8402422451346, 691.8298638166553, 691.819486299019, 691.8091096921404, 691.7987339959419, 691.7883592103385, 691.7779853352548, 691.7676123706057, 691.7572403163116, 691.7468691722912, 691.736498938464, 691.7261296147485, 691.7157612010641, 691.7053936973298, 691.6950271034643, 691.6846614193879, 691.674296645017, 691.6639327802735, 691.6535698250759, 691.6432077793423, 691.6328466429926, 691.6224864159462, 691.6121270981224, 691.6017686894396, 691.5914111898174, 691.5810545991748, 691.57069891743, 691.5603441445032, 691.5499902803141, 691.5396373247796, 691.5292852778226, 691.518934139359, 691.5085839093099, 691.4982345875937, 691.4878861741295, 691.4775386688372, 691.4671920716332, 691.4568463824434, 691.4465016011812, 691.4361577277654, 691.42581476212, 691.4154727041606, 691.4051315538082, 691.3947913109806, 691.3844519755972, 691.3741135475793, 691.3637760268448, 691.3534394133125, 691.3431037069046, 691.3327689075354, 691.3224350151298, 691.3121020296018, 691.3017699508749, 691.2914387788655, 691.2811085134954, 691.2707791546832, 691.2604507023472, 691.2501231564087, 691.2397965167848, 691.2294707833971, 691.2191459561647, 691.2088220350048, 691.1984990198398, 691.1881769105862, 691.1778557071651, 691.1675354094956, 691.1572160174996, 691.1468975310928, 691.1365799501956, 691.1262632747284, 691.1159475046097, 691.1056326397617, 691.0953186800998, 691.0850056255457, 691.0746934760218, 691.0643822314408, 691.0540718917259, 691.0437624567999, 691.0334539265765, 691.0231463009794, 691.0128395799253, 691.0025337633373, 690.9922288511301, 690.9819248432283, 690.9716217395479, 690.9613195400099, 690.9510182445324, 690.9407178530379, 690.9304183654435, 690.9201197816718, 690.9098221016385, 690.8995253252662, 690.889229452472, 690.8789344831785, 690.8686404173031, 690.8583472547672, 690.8480549954894, 690.8377636393872, 690.8274731863845, 690.8171836363985, 690.80689498935, 690.7966072451582, 690.786320403742, 690.7760344650229, 690.7657494289195, 690.75546529535, 690.7451820642369, 690.7348997354997, 690.7246183090564, 690.7143377848275, 690.7040581627316, 690.6937794426925, 690.6835016246259, 690.6732247084524, 690.6629486940928, 690.6526735814667, 690.6423993704947, 690.6321260610922, 690.6218536531854, 690.6115821466897, 690.6013115415282, 690.5910418376178, 690.58077303488, 690.570505133233, 690.5602381325981, 690.5499720328943, 690.5397068340459, 690.5294425359651, 690.5191791385759, 690.5089166417985, 690.4986550455527, 690.4883943497598, 690.4781345543338, 690.467875659201, 690.4576176642812, 690.4473605694885, 690.4371043747489, 690.4268490799776, 690.4165946850991, 690.406341190031, 690.396088594695, 690.3858368990077, 690.3755861028895, 690.3653362062646, 690.3550872090492, 690.3448391111666, 690.3345919125309, 690.3243456130696, 690.3141002126973, 690.3038557113358, 690.2936121089056, 690.2833694053267, 690.2731276005194, 690.2628866943996, 690.2526466868948, 690.242407577919, 690.2321693673966, 690.2219320552452, 690.2116956413837, 690.2014601257357, 690.1912255082192, 690.1809917887541, 690.1707589672602, 690.1605270436611, 690.1502960178715, 690.1400658898151, 690.1298366594127, 690.1196083265821, 690.1093808912455, 690.0991543533211, 690.0889287127334, 690.0787039693945, 690.0684801232328, 690.058257174163, 690.0480351221098, 690.0378139669903, 690.0275937087223, 690.0173743472325, 690.0071558824385, 689.9969383142587, 689.9867216426155, 689.9765058674293, 689.966290988616, 689.9560770061031, 689.9458639198045, 689.9356517296452, 689.9254404355419, 689.9152300374167, 689.9050205351914, 689.8948119287825, 689.8846042181121, 689.8743974031009, 689.8641914836725, 689.8539864597407, 689.8437823312288, 689.8335790980603, 689.8233767601508, 689.8131753174239, 689.8029747697997, 689.7927751171934, 689.782576359533, 689.7723784967353, 689.7621815287215, 689.7519854554113, 689.7417902767253, 689.7315959925824, 689.7214026029069, 689.711210107616, 689.7010185066318, 689.6908277998738, 689.6806379872622, 689.6704490687212, 689.6602610441663, 689.6500739135178, 689.6398876767012, 689.6297023336319, 689.6195178842355, 689.6093343284291, 689.5991516661305, 689.5889698972663, 689.5787890217542, 689.5686090395167, 689.5584299504688, 689.5482517545362, 689.5380744516405, 689.5278980416982, 689.5177225246308, 689.5075479003609, 689.4973741688076, 689.4872013298927, 689.4770293835342, 689.4668583296542, 689.4566881681752, 689.4465188990152, 689.4363505220967, 689.4261830373398, 689.4160164446638, 689.4058507439913, 689.3956859352402, 689.3855220183362, 689.3753589931944, 689.3651968597393, 689.3550356178912, 689.3448752675678, 689.334715808694, 689.324557241188, 689.3143995649706, 689.3042427799625, 689.2940868860845, 689.2839318832589, 689.2737777714054, 689.2636245504442, 689.253472220296, 689.2433207808825, 689.233170232125, 689.2230205739439, 689.2128718062581, 689.2027239289891, 689.1925769420599, 689.182430845391, 689.1722856388992, 689.1621413225101, 689.1519978961411, 689.1418553597176, 689.1317137131556, 689.1215729563777, 689.1114330893064, 689.1012941118597, 689.091156023961, 689.0810188255283, 689.0708825164861, 689.0607470967539, 689.0506125662513, 689.0404789249005, 689.0303461726198, 689.0202143093333, 689.010083334964, 688.9999532494286, 688.9898240526489, 688.9796957445463, 688.9695683250426, 688.9594417940554, 688.9493161515121, 688.9391913973272, 688.9290675314256, 688.9189445537271, 688.9088224641536, 688.8987012626222, 688.8885809490588, 688.8784615233827, 688.8683429855142, 688.8582253353769, 688.8481085728882, 688.8379926979718, 688.8278777105476, 688.8177636105369, 688.8076503978587, 688.7975380724403, 688.7874266341963, 688.7773160830501, 688.7672064189223, 688.7570976417361, 688.7469897514114, 688.7368827478707, 688.7267766310293, 688.7166714008157, 688.706567057147, 688.6964635999458, 688.686361029132, 688.6762593446275, 688.6661585463544, 688.6560586342334, 688.645959608185, 688.6358614681297, 688.62576421399, 688.6156678456882, 688.6055723631445, 688.5954777662778, 688.5853840550135, 688.5752912292686, 688.5651992889685, 688.5551082340309, 688.5450180643802, 688.534928779936, 688.5248403806173, 688.5147528663484, 688.5046662370511, 688.4945804926447, 688.484495633052, 688.4744116581957, 688.4643285679908, 688.4542463623662, 688.4441650412392, 688.4340846045308, 688.4240050521653, 688.4139263840599, 688.4038486001417, 688.3937717003238, 688.3836956845353, 688.3736205526952, 688.3635463047249, 688.3534729405428, 688.3434004600741, 688.3333288632384, 688.3232581499584, 688.3131883201548, 688.3031193737498, 688.2930513106629, 688.282984130815, 688.2729178341338, 688.2628524205315, 688.2527878899367, 688.2427242422685, 688.2326614774491, 688.2225995953967, 688.212538596037, 688.2024784792902, 688.1924192450774, 688.18236089332, 688.1723034239402, 688.1622468368583, 688.1521911319948, 688.1421363092769, 688.132082368618, 688.1220293099476, 688.1119771331819, 688.1019258382435, 688.0918754250573, 688.0818258935386, 688.0717772436154, 688.0617294752055, 688.0516825882318, 688.0416365826153, 688.0315914582776, 688.0215472151416, 688.0115038531294, 688.0014613721601, 687.9914197721563, 687.9813790530379, 687.9713392147308, 687.9613002571555, 687.9512621802302, 687.9412249838794, 687.9311886680254, 687.92115323259, 687.9111186774918, 687.9010850026552, 687.8910522080042, 687.8810202934533, 687.8709892589303, 687.860959104355, 687.8509298296477, 687.8409014347327, 687.8308739195321, 687.8208472839641, 687.810821527955, 687.8007966514247, 687.7907726542932, 687.7807495364838, 687.7707272979184, 687.7607059385197, 687.7506854582069, 687.7406658569038, 687.7306471345328, 687.7206292910145, 687.7106123262696, 687.7005962402227, 687.6905810327947, 687.6805667039052, 687.6705532534792, 687.6605406814373, 687.6505289877019, 687.6405181721934, 687.6305082348359, 687.6204991755504, 687.6104909942577, 687.6004836908793, 687.5904772653403, 687.5804717175606, 687.57046704746, 687.5604632549662, 687.5504603399958, 687.5404583024747, 687.5304571423194, 687.5204568594571, 687.5104574538078, 687.5004589252936, 687.4904612738375, 687.480464499361, 687.4704686017827, 687.4604735810317, 687.4504794370224, 687.4404861696826, 687.4304937789312, 687.4205022646889, 687.4105116268848, 687.4005218654329, 687.3905329802587, 687.380544971285, 687.3705578384347, 687.3605715816257, 687.3505862007833, 687.3406016958285, 687.3306180666847, 687.3206353132729, 687.3106534355157, 687.3006724333336, 687.2906923066508, 687.2807130553882, 687.2707346794701, 687.2607571788157, 687.2507805533493, 687.2408048029938, 687.2308299276684, 687.2208559272962, 687.2108828017987, 687.200910551102, 687.1909391751255, 687.1809686737902, 687.1709990470207, 687.1610302947375, 687.1510624168645, 687.1410954133212, 687.1311292840329, 687.1211640289216, 687.1111996479067, 687.1012361409126, 687.0912735078623, 687.0813117486779, 687.071350863279, 687.0613908515884, 687.0514317135348, 687.0414734490317, 687.0315160580076, 687.0215595403793, 687.0116038960747, 687.0016491250134, 686.9916952271177, 686.9817422023076, 686.9717900505138, 686.96183877165, 686.951888365641, 686.9419388324089, 686.9319901718794, 686.9220423839719, 686.9120954686082, 686.9021494257131, 686.8922042552081, 686.8822599570149, 686.8723165310561, 686.8623739772542, 686.8524322955327, 686.8424914858106, 686.8325515480159, 686.8226124820654, 686.8126742878859, 686.8027369653997, 686.7928005145255, 686.7828649351876, 686.7729302273116, 686.7629963908144, 686.7530634256251, 686.7431313316596, 686.7332001088453, 686.7232697571013, 686.7133402763521, 686.7034116665205]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot cost versus iteration\n",
    "\n",
    "# plt.subplots(number_of_graphs_rows, number_of_graphs_columns, constrained_layout=True, figsize=(row, hight))\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "\n",
    "# will plot the cost history complete\n",
    "ax1.plot(J_hist)\n",
    "\n",
    "# J_hist[100:] will return first 100 elements\n",
    "# will plot the last values for cost\n",
    "ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "\n",
    "# adding title for the 2 charts\n",
    "ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "\n",
    "# adding y label for the 2 charts \"Cost\"\n",
    "ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \n",
    "\n",
    "# adding x label for the 2 charts \"iteration step\"\n",
    "ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step')\n",
    "\n",
    "# shows the charts\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These results are not inspiring*! Cost is still declining and our predictions are not very accurate. The next lab will explore how to improve on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"toc_15456_6\"></a>\n",
    "# 6 Congratulations!\n",
    "In this lab you:\n",
    "- Redeveloped the routines for linear regression, now with multiple variables.\n",
    "- Utilized NumPy `np.dot` to vectorize the implementations"
   ]
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "15456"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
